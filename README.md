# ERC-Tech Stuff
Electronics and Robotics Club Posts -

## TechThursdays-
### Kilobots-
How does a group of animals -- or cells, for that matter -- work together when no one’s in charge? Tiny swarming robots--called Kilobots--work together to tackle tasks in the lab, but what can they teach us about the natural world?
About Kilobots
How do you simultaneously control a thousand robots in a swarm? The question may seem like science fiction, but it’s one that has challenged real robotics engineers for decades.
In 2010, the Kilobot entered the scene. Now, engineers are programming these tiny independent robots to cooperate on group tasks. This research could one day lead to robots that can assemble themselves into machines, or provide insights into how swarming behaviors emerge in nature.
In the future, this kind of research might lead to collaborative robots that could self-assemble into a composite structure. This larger robot could work in dangerous or contaminated areas, like cleaning up oil spills or conducting search-and-rescue activities.
What is Emergent Behavior?
The universe tends towards chaos, but sometimes patterns emerge, like a flock of birds in flight. Like termites building skyscrapers out of mud, or fish schooling to avoid predators.
It’s called emergent behavior. Complex behaviors that arise from interactions between simple things. And you don’t just see it in nature.
What’s so interesting about kilobots is that individually, they’re pretty dumb.
They’re designed to be simple. A single kilobot can do maybe... three things: Respond to light. Measure a distance, sense the presence of other kilobots. 
But these are swarm robots. They work together.
How do Kilobots work?
Kilobots were designed by Michael Rubenstein, a research scientist in the Self Organizing Systems Research Group at Harvard. Each robot consists of about $15 worth of parts: a microprocessor that is about as smart as a calculator, sensors for visible and infrared light, and two tiny cell-phone vibration units that allow it to move across a table. They are powered by a rechargeable lithium-ion battery, like those found in small electronics or watches.
The kilobots are programmed all at once, as a group, using infrared light. Each kilobot gets the same set of instructions as the next. With just a few lines of programming, the kilobots, together, can act out complex natural processes.
The same kinds of simple instructions that kilobots use to self-assemble into shapes can make them mimic natural swarming behaviors, too. For example, kilobots can sync their flashing lights like a swarm of fireflies, differentiate similar to cells in an embryo and follow a scent trail like foraging ants.

for more interesting stuff about kilobots,click [here](https://youtu.be/QXNVZJ3KUsA)

## TechThursdays
### LOVOTS-
LOVOT : A Combination of Love+Robot!
Want to be loved by someone? Want to enhance your mood?
Here comes your companion named 'Lovot', an excellent example of Emotional Robotics!

Lovot will react to your moods, has facial expressions and do all it can to fill you with joy and re-energize you.This 4.2 kilogram weighing bot, a Japan based model, has 10 or more CPU cores, 20 or more MCUs, and 50 or more sensors, which can create behaviour that is much like a living being.
Lovot uses Machine-Learning to understand the behaviours of Human beings.An application in the mobile is used to keep its track, change its eye-color and to check its daily routine via a calender.Lovot's actions are pre-programmed through deep-learning, so that they can take actions in real time.There are foldable wheels, flexible shoulders and a smooth skin comprising of pressure sensors, 360-degree half sphere camera, thermal camera, posture sensors,etc to detect human touch and its responses.Lovot's reactions are cute and warming.

![lovot](https://github.com/sharvaree1921/ERC-Tech-/blob/master/images/lovot.jpg)

Want to know more? 
[click here](https://lovot.life/en/technology/)
  
Check out the video here-
1-[video 1](https://www.youtube.com/watch?v=PoxdaQzdx3I)
2-[video 2](https://www.youtube.com/watch?v=U_kijW18EVU)

### Sketch to Image
A team of researchers from the Chinese Academy of Sciences and the City University of Hong Kong has introduced a local-to-global approach that can generate lifelike human portraits from relatively rudimentary sketches.

Recent deep image-to-image translation techniques have enabled the prompt generation of human face images from sketches, but these methods tend to suffer from overfitting to their inputs. They thus achieve the most realistic results only when the source drawings have high-quality artistry or are accompanied by edge maps.

Unlike most deep learning based solutions for sketch-to-image translation that take input sketches as fixed, ‘hard’ constraints and then attempt to reconstruct the missing texture or shading information between strokes, the key idea behind the new approach is to implicitly learn a space of plausible face sketches from real face sketch images and find the point in this space that best approximates the input sketch. Because this approach treats input sketches more as ‘soft’ constraints that will guide image synthesis, it is able to produce high-quality face images with increased plausibility even from rough and/or incomplete inputs.

The system consists of three main modules — CE (Component Embedding), FM (Feature Mapping), and IS (Image Synthesis). The CE module adopts an auto-encoder architecture and separately learns five feature descriptors — left-eye, right-eye, nose, mouth, and remainder — from the face sketch data. The FM and IS modules together form another deep learning sub-network for conditional image generation, and map component feature vectors to realistic images.

The researchers also provide a shadow-guided interface, implemented based on CE, that makes it easier for users to refine the input sketches. Their system can produce high-quality realistic face images — with resolution of 512 × 512 — that faithfully respect and reflect the input sketches.

Both qualitative and quantitative evaluations show that the method produces visually more pleasing face images, according to the researchers. The system’s usability and expressiveness were also favourably confirmed in a user study.

The researchers say their tool is easy to use, even for non-artists, while still supporting fine-grained control of shape details. They are working on releasing the source code soon.

![dl1](https://github.com/sharvaree1921/ERC-Tech-/blob/master/images/dl1.jpg)
![dl2](https://github.com/sharvaree1921/ERC-Tech-/blob/master/images/dl2.png)

